{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc62dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".option(\"inferSchema\", \"true\")\\\n",
    ".load(\"./data/retail-data/by-day/2010-12-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ef4968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e59901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[5: int, five: string, 5.0: double]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 스파크 데이터 타입으로 변환\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04767147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------+\n",
      "|InvoiceNo|Description                  |\n",
      "+---------+-----------------------------+\n",
      "|536366   |HAND WARMER UNION JACK       |\n",
      "|536366   |HAND WARMER RED POLKA DOT    |\n",
      "|536367   |ASSORTED COLOUR BIRD ORNAMENT|\n",
      "|536367   |POPPY'S PLAYHOUSE BEDROOM    |\n",
      "|536367   |POPPY'S PLAYHOUSE KITCHEN    |\n",
      "+---------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. 불리언\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.where(col(\"InvoiceNo\")!= 536365).select(\"InvoiceNo\", \"Description\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca342546",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'false' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/50/34qx4w51577d2hlff47g2fp00000gn/T/ipykernel_76816/3234530521.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"InvoiceNo = 536365\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"InvoiceNo <> 536365\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'false' is not defined"
     ]
    }
   ],
   "source": [
    "df.where(\"InvoiceNo = 536365\").show(5, false)\n",
    "df.where(\"InvoiceNo <> 536365\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b85e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and는 차례대로 필터를 적용\n",
    "# or는 반드시 동일 구문\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "priceFilter = col('UnitPrice')> 600\n",
    "descripFilter = instr(df.Description, \"POSTAGE\") >= 1\n",
    "df.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 수치형 데이터\n",
    "\n",
    "from pyspark.sql.functions import expr, pow\n",
    "\n",
    "fabricatedQuantity = pow(col('Quantity')* col(\"UnitPrice\"), 2) + 5\n",
    "df.select(expr(\"CustomerId\"), fabricatedQuantity.alias('realQuantity')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97c824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d7989",
   "metadata": {},
   "outputs": [],
   "source": [
    "olName = \"UnitPrice\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "\n",
    "df.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9b3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.stat.crosstab(\"StockCode\", \"Quantity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e617cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 문자열 \n",
    "\n",
    "from pyspark.sql.functions import initcap\n",
    "\n",
    "df.select(initcap(col(\"Description\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e0bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, upper\n",
    "\n",
    "df.select(col(\"Description\"),lower(col(\"Description\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a071157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import reqexp_replace\n",
    "\n",
    "regex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "df.select(reqexp_replace(\"Description\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0cddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 날짜 timestamp\n",
    "\n",
    "from pyspark.sql.functions import current_date,  current_timestamp\n",
    "\n",
    "dateDF = spark.range(10)\\\n",
    ".withColumn(\"today\", current_date())\\\n",
    ".withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43f6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null 다루기\n",
    "\n",
    "# coalesce: 인수로 지정한 여러 컬럼 중 null이 아닌 첫번째 값 반환\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "df.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255146c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구조체\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "complexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\n",
    "complexDF.createOrReplaceTempView(\"complexDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fefe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complexDF.select(\"complex.Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a36c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배열\n",
    "\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(split(col(\"Description\"), \" \")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON\n",
    "\n",
    "jsonDF = spark.range(1).selectExpr(\"\"\"\n",
    "    '{\"myJSONKey\": {\"myJSONValue\" : [1,2,3]}}' as jsonString\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd65b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc76936",
   "metadata": {},
   "outputs": [],
   "source": [
    "udfExampleDF = spark.raange(5).toDF(\"num\")\n",
    "\n",
    "def power3(double_value):\n",
    "    return double_value ** 3\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b0cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "power3udf = udf(power3)\n",
    "\n",
    "udfExampleDF.selectExpr(\"power3(num)\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a818e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "cleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "574ab907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|to_timestamp(2021-12-28, yyyy-MM-dd)|\n",
      "+------------------------------------+\n",
      "|                 2021-12-28 00:00:00|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateFormat = \"yyyy-MM-dd\"\n",
    "spark.range(1).select(to_timestamp(lit(\"2021-12-28\"), dateFormat)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0015ede7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
